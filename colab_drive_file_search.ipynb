{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "Google AI Studio — Drive File Search"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google AI Studio — Drive File Search\n",
    "\n",
    "Full-text search across **1 000+ extensionless files** stored in your\n",
    "Google Drive `/Google AI Studio` folder.\n",
    "\n",
    "---\n",
    "\n",
    "## Expert Analysis & Critique\n",
    "\n",
    "### Problem characteristics\n",
    "\n",
    "| Constraint | Implication |\n",
    "|---|---|\n",
    "| 1 000+ files | API pagination required (`pageSize` max 1000) |\n",
    "| No file extension | Cannot filter by extension; must rely on MIME metadata + binary heuristics |\n",
    "| Mixed text / binary content | Must detect and skip binary *before* indexing |\n",
    "| Colab environment | ~12 GB RAM, ephemeral runtime, network-bound I/O |\n",
    "\n",
    "### Approach comparison\n",
    "\n",
    "| Method | Throughput | Pre-filtering | Complexity |\n",
    "|---|---|---|---|\n",
    "| `drive.mount()` + `os.walk` | **Slow** — FUSE adds per-file latency | None — must open every file | Trivial |\n",
    "| Drive API, sequential downloads | Slow — one HTTP round-trip at a time | MIME-based | Low |\n",
    "| **Drive API + parallel downloads** | **~20-30× faster** — concurrent I/O | **MIME + binary heuristics** | Moderate |\n",
    "| Google Cloud Search / Vertex AI Search | Very fast | Built-in | Requires Workspace admin / costs |\n",
    "\n",
    "### Chosen architecture\n",
    "\n",
    "```\n",
    "Auth ➜ List files (paginated) ➜ Filter by MIME\n",
    "     ➜ Parallel download (ThreadPoolExecutor, 25 workers)\n",
    "     ➜ Binary detection (null-byte heuristic on first 8 KB)\n",
    "     ➜ UTF-8 / Latin-1 decode\n",
    "     ➜ In-memory dict  { filename : text }\n",
    "     ➜ Regex / substring search ➜ Highlighted results\n",
    "```\n",
    "\n",
    "**Why this wins:**\n",
    "\n",
    "1. **MIME pre-filter** — `image/*`, `video/*`, `audio/*`, `application/zip`,\n",
    "   and native Google types (`application/vnd.google-apps.*`) are skipped\n",
    "   *without downloading a single byte*.\n",
    "2. **Parallel I/O** — 25 concurrent workers saturate the network link instead\n",
    "   of waiting sequentially. For 1 000 files this is the single biggest speedup.\n",
    "3. **Binary detection** — Files typed as `application/octet-stream` (common\n",
    "   for extensionless files) get a fast null-byte scan on the first 8 KB before\n",
    "   the full content is decoded.\n",
    "4. **In-memory index** — After the one-time download pass, every subsequent\n",
    "   search is pure in-memory string/regex matching—instant.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "| Limitation | Mitigation |\n",
    "|---|---|\n",
    "| ~12 GB RAM ceiling | 1 000 text files ≈ 50–200 MB — well within budget |\n",
    "| Drive API quota (12 000 req/min) | 25 workers × 1 000 files = 1 000 calls, ~8 % of quota |\n",
    "| Index lost on runtime restart | Re-run the indexing cell; optionally pickle the index |\n",
    "| Keyword search only (no semantics) | Regex support covers complex patterns; add embeddings later if needed |\n",
    "| Google-native docs (Docs/Sheets) not downloaded | These require `export()`; add a dedicated cell if needed |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Setup & Authentication ───────────────────────────────────────────\n",
    "\n",
    "from google.colab import auth\n",
    "import google.auth\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "import io, re, time, csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "\n",
    "drive = build('drive', 'v3', credentials=creds)\n",
    "print('\\u2713 Authenticated \\u2014 Drive API ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these knobs before running the indexing cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Configuration ───────────────────────────────────────────────────\n",
    "\n",
    "FOLDER_NAME    = 'Google AI Studio'   # Folder name (at Drive root)\n",
    "MAX_WORKERS    = 25                    # Parallel download threads\n",
    "MAX_FILE_SIZE  = 10 * 1024 * 1024     # Skip files larger than 10 MB\n",
    "CONTEXT_LINES  = 2                     # Lines of context around each match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Engine\n",
    "\n",
    "Run this cell once to define all helper functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Core Engine ─────────────────────────────────────────────────────\n",
    "\n",
    "# MIME prefixes that are guaranteed non-text — skip without downloading\n",
    "_SKIP_MIME = (\n",
    "    'image/', 'video/', 'audio/',\n",
    "    'application/zip', 'application/x-tar', 'application/x-rar',\n",
    "    'application/pdf', 'application/vnd.google-apps.',\n",
    ")\n",
    "\n",
    "\n",
    "def find_folder_id(name, parent='root'):\n",
    "    \"\"\"Resolve a folder name under *parent* to its Drive ID.\"\"\"\n",
    "    q = (f\"name='{name}' and '{parent}' in parents and \"\n",
    "         f\"mimeType='application/vnd.google-apps.folder' and trashed=false\")\n",
    "    resp = drive.files().list(q=q, fields='files(id,name)',\n",
    "                              pageSize=5).execute()\n",
    "    hits = resp.get('files', [])\n",
    "    if not hits:\n",
    "        raise FileNotFoundError(f\"Folder '{name}' not found under parent='{parent}'\")\n",
    "    return hits[0]['id']\n",
    "\n",
    "\n",
    "def list_files(folder_id):\n",
    "    \"\"\"List every non-folder file inside *folder_id* (handles pagination).\"\"\"\n",
    "    files, token = [], None\n",
    "    while True:\n",
    "        resp = drive.files().list(\n",
    "            q=(f\"'{folder_id}' in parents and trashed=false and \"\n",
    "               f\"mimeType!='application/vnd.google-apps.folder'\"),\n",
    "            fields='nextPageToken,files(id,name,mimeType,size)',\n",
    "            pageSize=1000,\n",
    "            pageToken=token,\n",
    "        ).execute()\n",
    "        files.extend(resp.get('files', []))\n",
    "        token = resp.get('nextPageToken')\n",
    "        if not token:\n",
    "            break\n",
    "    return files\n",
    "\n",
    "\n",
    "def _is_binary(data: bytes) -> bool:\n",
    "    \"\"\"Fast heuristic: null bytes or high ratio of control chars means binary.\"\"\"\n",
    "    sample = data[:8192]\n",
    "    if b'\\x00' in sample:\n",
    "        return True\n",
    "    non_text = sum(1 for b in sample if b < 8 or (13 < b < 32))\n",
    "    return non_text / max(len(sample), 1) > 0.10\n",
    "\n",
    "\n",
    "def download(finfo):\n",
    "    \"\"\"Download one file. Returns (name, text|None, status_tag).\"\"\"\n",
    "    fid   = finfo['id']\n",
    "    name  = finfo['name']\n",
    "    mime  = finfo.get('mimeType', '')\n",
    "    size  = int(finfo.get('size', 0))\n",
    "\n",
    "    # ── Pre-filters ────────────────────────────────────────\n",
    "    if any(mime.startswith(p) for p in _SKIP_MIME):\n",
    "        return name, None, 'skip_mime'\n",
    "    if size > MAX_FILE_SIZE:\n",
    "        return name, None, 'skip_size'\n",
    "\n",
    "    # ── Download with retry ────────────────────────────────\n",
    "    for attempt in range(4):\n",
    "        try:\n",
    "            buf = io.BytesIO()\n",
    "            dl  = MediaIoBaseDownload(buf, drive.files().get_media(fileId=fid))\n",
    "            done = False\n",
    "            while not done:\n",
    "                _, done = dl.next_chunk()\n",
    "            raw = buf.getvalue()\n",
    "\n",
    "            if not raw:\n",
    "                return name, None, 'empty'\n",
    "            if _is_binary(raw):\n",
    "                return name, None, 'binary'\n",
    "\n",
    "            # Decode: prefer UTF-8, fall back to Latin-1 (never fails)\n",
    "            try:\n",
    "                return name, raw.decode('utf-8'), 'ok'\n",
    "            except UnicodeDecodeError:\n",
    "                return name, raw.decode('latin-1'), 'ok'\n",
    "\n",
    "        except HttpError as e:\n",
    "            if e.resp.status in (429, 500, 503) and attempt < 3:\n",
    "                time.sleep(2 ** attempt)\n",
    "                continue\n",
    "            return name, None, f'http_{e.resp.status}'\n",
    "        except Exception as exc:\n",
    "            return name, None, 'error'\n",
    "\n",
    "    return name, None, 'max_retries'\n",
    "\n",
    "\n",
    "def search(index, query, case_insensitive=True, use_regex=False,\n",
    "           context=CONTEXT_LINES):\n",
    "    \"\"\"Search the in-memory index. Returns [(name, count, [context_blocks])].\"\"\"\n",
    "    flags = re.IGNORECASE if case_insensitive else 0\n",
    "    pat = re.compile(query if use_regex else re.escape(query), flags)\n",
    "\n",
    "    results = []\n",
    "    for name, content in index.items():\n",
    "        lines = content.split('\\n')\n",
    "        hits  = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if pat.search(line):\n",
    "                s = max(0, i - context)\n",
    "                e = min(len(lines), i + context + 1)\n",
    "                block = []\n",
    "                for j in range(s, e):\n",
    "                    marker = '\\u25b6' if j == i else ' '\n",
    "                    block.append(f'{marker} {j+1:>5} \\u2502 {lines[j]}')\n",
    "                hits.append('\\n'.join(block))\n",
    "        if hits:\n",
    "            results.append((name, len(hits), hits))\n",
    "\n",
    "    results.sort(key=lambda r: -r[1])\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_results(results, max_hits_per_file=5, max_files=30):\n",
    "    \"\"\"Pretty-print search results to stdout.\"\"\"\n",
    "    if not results:\n",
    "        print('No matches found.')\n",
    "        return\n",
    "    total = sum(c for _, c, _ in results)\n",
    "    print(f'Matches in {len(results)} file(s)  ({total} total hits)\\n')\n",
    "    for name, count, hits in results[:max_files]:\n",
    "        label = f'{count} match' + ('es' if count != 1 else '')\n",
    "        print(f'\\u2501\\u2501\\u2501 {name}  ({label}) \\u2501\\u2501\\u2501')\n",
    "        for h in hits[:max_hits_per_file]:\n",
    "            print(h)\n",
    "            print()\n",
    "        if count > max_hits_per_file:\n",
    "            print(f'    \\u2026 and {count - max_hits_per_file} more match(es)\\n')\n",
    "    if len(results) > max_files:\n",
    "        print(f'\\u2026 and {len(results) - max_files} more file(s) with matches.')\n",
    "\n",
    "\n",
    "print('\\u2713 Core functions defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Index\n",
    "\n",
    "Run this cell **once** per session. It lists all files, downloads text\n",
    "content in parallel, and builds the in-memory search index."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Build Index ─────────────────────────────────────────────────────\n",
    "\n",
    "folder_id = find_folder_id(FOLDER_NAME)\n",
    "all_files = list_files(folder_id)\n",
    "print(f'Found {len(all_files):,} files in /{FOLDER_NAME}/\\n')\n",
    "\n",
    "index = {}\n",
    "stats = {'ok': 0, 'skip_mime': 0, 'skip_size': 0,\n",
    "         'binary': 0, 'empty': 0, 'errors': 0}\n",
    "\n",
    "t0 = time.time()\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:\n",
    "    futures = {pool.submit(download, f): f for f in all_files}\n",
    "    done_count = 0\n",
    "    for fut in as_completed(futures):\n",
    "        done_count += 1\n",
    "        name, content, status = fut.result()\n",
    "        if status == 'ok':\n",
    "            index[name] = content\n",
    "            stats['ok'] += 1\n",
    "        elif status in stats:\n",
    "            stats[status] += 1\n",
    "        else:\n",
    "            stats['errors'] += 1\n",
    "        if done_count % 100 == 0 or done_count == len(all_files):\n",
    "            print(f'\\r  Progress: {done_count:,}/{len(all_files):,}', end='', flush=True)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "mem_mb  = sum(len(v) for v in index.values()) / 1_048_576\n",
    "\n",
    "print(f'\\n\\n{\"=\"*45}')\n",
    "print(f'  Text files indexed : {stats[\"ok\"]:,}')\n",
    "print(f'  Skipped (MIME)     : {stats[\"skip_mime\"]:,}')\n",
    "print(f'  Skipped (size)     : {stats[\"skip_size\"]:,}')\n",
    "print(f'  Skipped (binary)   : {stats[\"binary\"]:,}')\n",
    "print(f'  Skipped (empty)    : {stats[\"empty\"]:,}')\n",
    "print(f'  Errors             : {stats[\"errors\"]:,}')\n",
    "print(f'  Index memory       : {mem_mb:.1f} MB')\n",
    "print(f'  Elapsed            : {elapsed:.1f} s')\n",
    "print(f'{\"=\"*45}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Edit `QUERY` below and re-run this cell as many times as you like.\n",
    "The index stays in memory — searches are instant."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Search ──────────────────────────────────────────────────────────\n",
    "\n",
    "QUERY            = 'your search term here'   # <── EDIT THIS\n",
    "CASE_INSENSITIVE = True\n",
    "USE_REGEX        = False   # Set True for regex patterns (e.g. r'model.*temperature')\n",
    "\n",
    "results = search(index, QUERY, CASE_INSENSITIVE, USE_REGEX)\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Search (widget)\n",
    "\n",
    "An interactive text box so you don't have to re-run a cell for each query."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Interactive Widget Search ────────────────────────────────────────\n",
    "\n",
    "query_box    = widgets.Text(placeholder='Enter search query\\u2026',\n",
    "                            layout=widgets.Layout(width='50%'))\n",
    "case_toggle  = widgets.Checkbox(value=True,  description='Case insensitive')\n",
    "regex_toggle = widgets.Checkbox(value=False, description='Regex')\n",
    "search_btn   = widgets.Button(description='Search', button_style='primary')\n",
    "out          = widgets.Output()\n",
    "\n",
    "def _on_search(_=None):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        q = query_box.value.strip()\n",
    "        if not q:\n",
    "            print('Enter a search query.')\n",
    "            return\n",
    "        r = search(index, q, case_toggle.value, regex_toggle.value)\n",
    "        print_results(r)\n",
    "\n",
    "search_btn.on_click(_on_search)\n",
    "query_box.on_submit(_on_search)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([query_box, search_btn]),\n",
    "    widgets.HBox([case_toggle, regex_toggle]),\n",
    "    out\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to CSV (optional)\n",
    "\n",
    "Run a search first (the `results` variable from the manual search cell),\n",
    "then execute this cell to download a CSV summary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ── Export to CSV ───────────────────────────────────────────────────\n",
    "\n",
    "csv_path = '/content/search_results.csv'\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['File', 'Matches', 'First match preview'])\n",
    "    for name, count, hits in results:\n",
    "        preview = hits[0].replace('\\n', ' | ')[:300] if hits else ''\n",
    "        w.writerow([name, count, preview])\n",
    "\n",
    "from google.colab import files\n",
    "files.download(csv_path)\n",
    "print(f'Exported {len(results)} rows to {csv_path}')"
   ]
  }
 ]
}